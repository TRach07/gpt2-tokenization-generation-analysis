{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TRach07/gpt2-tokenization-generation-analysis/blob/main/GPT2_Tokenization_Generation_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Practical Work: Pre-trained Language Model Manipulation**\n",
        "\n",
        "- **Objective** : Manipulate a pre-trained language model to understand text generation and explore the tokenization process.\n",
        "\n",
        "- **Tools**: Python, Hugging Face Transformers, PyTorch"
      ],
      "metadata": {
        "id": "o6bJ4Me48HXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Installation and Library Import**"
      ],
      "metadata": {
        "id": "JghHD--D8x8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0pZFEzgR7t2H",
        "outputId": "a923986b-1eea-4f88-8104-bc8797bd2979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU_YFPn887H1",
        "outputId": "0ae8aada-e01e-4dde-8c42-aa3cf6e67d18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n",
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Loading a Pre-trained Language Model**"
      ],
      "metadata": {
        "id": "cnjGjkbI9NJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # Using the base GPT-2 model\n",
        "\n",
        "print(\"Loading GPT-2 model and tokenizer...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "print(\"Model and tokenizer loaded successfully!\")\n",
        "\n",
        "# Display model information\n",
        "print(f\"\\nModel: {model_name}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "5342ccfcb2c84814b7ffc16b07849b8c",
            "8f20740332f84fd9a8afe34c8fb4737f",
            "6304f6993b6946879e8e40dcc0fc2cab",
            "debaceb0433d46c089050654d645e547",
            "b155ff6696d246698721aed6a3dd0923",
            "a30965e10cf34afaa91918f59fe339ca",
            "0400ca610ade429d9e70234da4812dbe",
            "10f2befa47914c56b88dd522c1c5617e",
            "254744fcff7246a0af276664e6182829",
            "98ff037de6624da2b2636476a9147173",
            "0418726ce1294cc6974c0268c9027324",
            "0338d3e27fc64040ac39cbd8ac0500b0",
            "f4c7a71d9e7f4cbc889746192fcfa678",
            "863e00fb68b94128a19e3205e46c0524",
            "76cf353ec7694c2db70bcc011602d018",
            "327530520c59439297f13d3d8f6fc894",
            "344fec54746946eda14bac0032be9b7a",
            "d885ba44a44749a08a744e092348d955",
            "1572bb675a4f4812904577b446d01860",
            "41b9173111ac424bbb98e055676b49e3",
            "8496fa5344eb41d8aae518f0251a8b35",
            "5c458647dc5d45ec8c52af3f17129a70",
            "9e7f361fdd9448bc8272651bf19147ef",
            "5489bc7ec0324078be0fa7dc7479c9e2",
            "743d7cf8bb8e4f3d81e50d1fbdd0c826",
            "4cd6360049434a8ba4fe637a86a21ecd",
            "f4fa970eda9c4519933e07025b77f93f",
            "b66ddbfb98d14dde90776b0e2125773e",
            "8154f076bcf4457faa70c283d19d8b47",
            "8a16fd1928394220b0912b4a9261eab7",
            "18f122cb47424790ac241524b0e9a4f0",
            "8db49ef9f1ec491ba1bc08f02fbe902d",
            "dc55695ffa9c4ce8a97a7f1748e76442",
            "71e26263a90640ab9e1400838be1fc12",
            "0e8a007c22fd4bac89990c521d999b9e",
            "0ff9e5eaa95644d5a277530c926e7c29",
            "ee6336e82800413bbdc5de722fabf6f5",
            "b109d44c345342a6af6f880755352914",
            "ae9b3ed554d94f089ebf8f8ef5377460",
            "1ae76c74f08a4b85901f63016f8236d9",
            "8f3fe0bd47d24883abcb36b8834a58b9",
            "1096684f384a4c3c9d4221a9ab7759f2",
            "26a09d3fcc114452986ec4c1352889c8",
            "c1890c7c2b984a1d8ac7fec3e4aab9da",
            "ce6f42ce820843f9a57df2ff864c708c",
            "b70c033cdb5c4952b66b58e7eb84c780",
            "447229a9cd254ac58fcaeecd839b1e55",
            "b4c1a4357cb54e93aedff979b52908cc",
            "c17d25efdab44c32a1a1084a598addd4",
            "b1e1d2bece84487ebde6a83b69a565f5",
            "d175904e37ea4a5d8f10758c2c86fa09",
            "37804af7634f40b194f6de460f950c8f",
            "8203ae0d034d471ca85b7a22e84371d7",
            "3370dc5109824035a170c5505abe221a",
            "28f00266d0f64501b6dd65a1622ec6d6",
            "db2a41677b044244844aaa28592ece86",
            "4fa5601b86974335940a4a7fd785bb22",
            "337484b00b1f4761b7f5f7dcc8b6fa3b",
            "530e353820ab4244a541b8a2322995b7",
            "9be0421d0707404da86075f9399c2e3f",
            "0daa4b4e0d0f4ab79cb7cb541d6c8a49",
            "230d26f4cce647d5a7992bf0f09c62fe",
            "5cc3d6bf64c443b396cd8eff47199018",
            "1276c1dab14e4c25966836c296241663",
            "7d26098d4e48408caacaaef58716af93",
            "b1d002d6a0f8431fb8111c181e93d09d",
            "eb9143320ff04298b9c5a0cad63046c6",
            "b83df8bea1794528a8fea038b6cfd153",
            "cce293f44a6d4f00a6346a03654eb53c",
            "54d4d6047357488b938164063215a109",
            "c1692e359e0142d6ac07d98ba4b06d35",
            "e85d2a8189de490bb21f3dd5cd5b6e98",
            "a9fc51f3bd2e479b9fea7a4d45838634",
            "b3ab703529cf435898d2a6dbcadaa6e7",
            "0e6c7356e37244e9ab4a3e0d350572cf",
            "e4c2b9c3ef814a47a680fb2c277b62c8",
            "ceb4abd4e95e4282a2ea62085c781f92"
          ]
        },
        "id": "ojXwWcIQ9ECM",
        "outputId": "55da5a7c-e2b9-48bc-904a-9e538fb9da6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GPT-2 model and tokenizer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5342ccfcb2c84814b7ffc16b07849b8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0338d3e27fc64040ac39cbd8ac0500b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e7f361fdd9448bc8272651bf19147ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71e26263a90640ab9e1400838be1fc12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce6f42ce820843f9a57df2ff864c708c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db2a41677b044244844aaa28592ece86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb9143320ff04298b9c5a0cad63046c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully!\n",
            "\n",
            "Model: gpt2\n",
            "Vocabulary size: 50257\n",
            "Model parameters: 124,439,808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Input Text Tokenization**"
      ],
      "metadata": {
        "id": "bWqA1gC29ovG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input text\n",
        "input_text = \"The artificial intelligence system\"\n",
        "\n",
        "print(\"Original text:\", input_text)\n",
        "\n",
        "# Tokenize the input text\n",
        "tokens = tokenizer.encode(input_text, return_tensors='pt')\n",
        "token_ids = tokens[0].tolist()\n",
        "\n",
        "print(\"\\n=== Tokenization Results ===\")\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "print(f\"Number of tokens: {len(token_ids)}\")\n",
        "\n",
        "# Display token to ID mapping\n",
        "print(\"\\nToken to ID mapping:\")\n",
        "for i, token_id in enumerate(token_ids):\n",
        "    token = tokenizer.decode([token_id])\n",
        "    print(f\"  Token {i+1}: ID={token_id}, Text='{token}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m06laxiA9W8d",
        "outputId": "bc9525ab-1067-431e-bcf6-d47487872d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: The artificial intelligence system\n",
            "\n",
            "=== Tokenization Results ===\n",
            "Token IDs: [464, 11666, 4430, 1080]\n",
            "Number of tokens: 4\n",
            "\n",
            "Token to ID mapping:\n",
            "  Token 1: ID=464, Text='The'\n",
            "  Token 2: ID=11666, Text=' artificial'\n",
            "  Token 3: ID=4430, Text=' intelligence'\n",
            "  Token 4: ID=1080, Text=' system'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Text Generation**"
      ],
      "metadata": {
        "id": "ZuHfwGNy-CYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text continuation\n",
        "print(\"=== Basic Text Generation ===\")\n",
        "print(f\"Input: '{input_text}'\")\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    generated_output = model.generate(\n",
        "        tokens,\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "# Decode and display generated text\n",
        "generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
        "print(f\"Generated: '{generated_text}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSGfQqLY98Dh",
        "outputId": "ca9f08ab-0ec1-4468-9e8b-6f308ae1e3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Basic Text Generation ===\n",
            "Input: 'The artificial intelligence system'\n",
            "Generated: 'The artificial intelligence system is able to predict the future, and it can also predict what the next generation of robots will look like.\n",
            "\n",
            "The system, called the AI-Robot-AI (AI-R) system (AAR), is'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Generation Parameters Exploration**"
      ],
      "metadata": {
        "id": "rmOuJVAf-SU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text with different parameters\n",
        "def generate_with_parameters(input_text, **kwargs):\n",
        "    \"\"\"Generate text with specified parameters\"\"\"\n",
        "    tokens = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_output = model.generate(\n",
        "            tokens,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(generated_output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "fg6RadOV-K3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5.1 Varying Temperature Parameter*"
      ],
      "metadata": {
        "id": "s4d00ApR-d69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Temperature Parameter Comparison ===\")\n",
        "print(f\"Input: '{input_text}'\\n\")\n",
        "\n",
        "temperatures = [0.1, 0.5, 1.0, 1.5]\n",
        "for temp in temperatures:\n",
        "    generated_text = generate_with_parameters(\n",
        "        input_text,\n",
        "        max_length=60,\n",
        "        temperature=temp,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    print(f\"Temperature {temp}: {generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct1eEsDe-Y4N",
        "outputId": "b24d09f7-d543-4c28-a065-49f89ac1e6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Temperature Parameter Comparison ===\n",
            "Input: 'The artificial intelligence system'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature 0.1: The artificial intelligence system is able to predict the future, and it can also predict the future in real time.\n",
            "\n",
            "The system is able to predict the future, and it can also predict the future in real time. The system is able to predict the future in real time. The system is able\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature 0.5: The artificial intelligence system is capable of making decisions about where to place a place, how to use the energy it generates, and how to use it in the future.\n",
            "\n",
            "The system is also capable of making decisions about who to send to a place. The system can make decisions about how to use\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature 1.0: The artificial intelligence system is the result of the researchers working on its artificial brains. So far, so good. But the researchers think they've found the exact right way to do that, and now they'll have a way to make that work in a system that's much more complicated than its computers.\n",
            "\n",
            "Temperature 1.5: The artificial intelligence system is a virtual organism with many thousands of neurons that work together. It may take 20-100 thousand artificial neurons at normal performance without any problems at all, although it might need much more of our human capabilities, just to put it simply, even if its memory processing would suffer,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5.2 Varying Top-k Parameter*"
      ],
      "metadata": {
        "id": "QC-U0-kY-uQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Top-k Parameter Comparison ===\")\n",
        "print(f\"Input: '{input_text}'\\n\")\n",
        "\n",
        "top_k_values = [5, 20, 50, 100]\n",
        "for top_k in top_k_values:\n",
        "    generated_text = generate_with_parameters(\n",
        "        input_text,\n",
        "        max_length=60,\n",
        "        top_k=top_k,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    print(f\"Top-k {top_k}: {generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXjjxlC6-mj0",
        "outputId": "18a2b1aa-fbaf-43d4-91de-a96d1812c1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top-k Parameter Comparison ===\n",
            "Input: 'The artificial intelligence system'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-k 5: The artificial intelligence system is designed to be a 'real' human, rather than just a 'computer,' but a 'computer' that can be 'computerized' and 'programmed,' which is what the artificial intelligence system does. It's not a 'computer,' because it's not. It\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-k 20: The artificial intelligence system in question is known as Machine Learning. This artificial intelligence system was developed by the University of Maryland's Artificial Intelligence Research Center and is based on the research in the field of artificial intelligence.\n",
            "\n",
            "In a statement, the company said it was aware of the problem and was \"working\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-k 50: The artificial intelligence system was able to predict the speed of its own movements and adjust it accordingly.\n",
            "\n",
            "The researchers said that it would be possible to build a system that could track and identify when a car is in motion.\n",
            "\n",
            "The machine would also be able to track and collect details about all\n",
            "\n",
            "Top-k 100: The artificial intelligence system is able to determine which of the two fields of interest is best suited for the job and which to leave in the hands of the AI-led group.\n",
            "\n",
            "\"We are still working on the first iteration of the AI, which will also have a small amount of human-\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5.3 Combined Parameters*"
      ],
      "metadata": {
        "id": "3ScCX0IO-4xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Combined Parameters ===\")\n",
        "print(f\"Input: '{input_text}'\\n\")\n",
        "\n",
        "# Test different combinations\n",
        "configurations = [\n",
        "    {\"temperature\": 0.3, \"top_k\": 10, \"max_length\": 50},\n",
        "    {\"temperature\": 0.8, \"top_k\": 50, \"max_length\": 50},\n",
        "    {\"temperature\": 1.2, \"top_k\": 100, \"max_length\": 50},\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configurations, 1):\n",
        "    generated_text = generate_with_parameters(\n",
        "        input_text,\n",
        "        do_sample=True,\n",
        "        **config\n",
        "    )\n",
        "    print(f\"Configuration {i} (temp={config['temperature']}, top_k={config['top_k']}):\")\n",
        "    print(f\"{generated_text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxMWwSrL-zuV",
        "outputId": "35be5ccf-b638-4601-a787-f9d32e75d616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Combined Parameters ===\n",
            "Input: 'The artificial intelligence system'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration 1 (temp=0.3, top_k=10):\n",
            "The artificial intelligence system can predict the future and predict the future. It can also predict the future and predict the future.\n",
            "\n",
            "The AI system can predict the future and predict the future. It can also predict the future and predict the future.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration 2 (temp=0.8, top_k=50):\n",
            "The artificial intelligence system was developed with the help of some of the most important people of the planet: the scientists who developed it, and the scientists who created it. Now, these scientists are back in the lab, working in a new way: they\n",
            "\n",
            "Configuration 3 (temp=1.2, top_k=100):\n",
            "The artificial intelligence system has been applied since 2003 to search Google,\" said Michael Martin, a consultant expert at CapitalBiz, which helps companies develop digital training services for defense analysts. \"The idea of this problem of trust at work has so far been\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Analysis Questions**"
      ],
      "metadata": {
        "id": "tInENqNP_EP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6.1 Parameter Effects Analysis*\n",
        "\n",
        "**Temperature Impact:**\n",
        "\n",
        "When temperature increases (0.1 → 1.5):\n",
        "\n",
        "- Low temperature (0.1): Produces highly deterministic and repetitive text. The model plays it safe, repeating phrases like \"predict the future\" multiple times. Output is coherent but lacks creativity.\n",
        "\n",
        "- Medium temperature (0.5-1.0): Strikes a balance between coherence and diversity. The text becomes more varied while maintaining logical flow.\n",
        "\n",
        "- High temperature (1.5): Generates highly creative but sometimes less coherent content. The model produces novel concepts like \"virtual organism with thousands of neurons\" but may lose focus.\n",
        "\n",
        "**Top-k Impact:**\n",
        "\n",
        "When top-k is reduced (100 → 5):\n",
        "\n",
        "- Low top-k (5): Severely limits vocabulary choices, resulting in constrained and repetitive generation. The model overuses certain words and patterns.\n",
        "\n",
        "- Medium top-k (20-50): Provides good diversity while maintaining relevance to the topic.\n",
        "\n",
        "- High top-k (100): Allows broad vocabulary selection, producing more varied and sometimes surprising content.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rNa-IlDyARQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6.2 Model Advantages and Limitations*\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "1. **Contextual Understanding**: The model correctly interprets \"artificial intelligence system\" and generates relevant content about prediction, machine learning, and AI research.\n",
        "\n",
        "2. **Coherent Text Structure**: Generates grammatically correct sentences with proper paragraph structure.\n",
        "\n",
        "3. **Knowledge Retention**: Demonstrates understanding of AI concepts like neural networks, prediction systems, and research applications.\n",
        "\n",
        "4. **Rapid Generation**: Produces lengthy, coherent text quickly without manual intervention.\n",
        "\n",
        "5. **Parameter Control**: Flexible adjustment of creativity vs. coherence through temperature and top-k parameters.\n",
        "\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "1. **Factual Accuracy**: May generate incorrect or fabricated information presented as fact (specific university names, system names).\n",
        "\n",
        "2. **Repetition Tendency**: Especially noticeable with low temperature settings, repeating phrases and ideas.\n",
        "\n",
        "3. **Context Drift**: Can gradually move away from the original topic over longer generations.\n",
        "\n",
        "4. **Parameter Sensitivity**: Small changes in parameters can produce significantly different results, requiring careful tuning.\n",
        "\n",
        "5. **Training Data Bias**: Reflects biases and knowledge cutoff from its training data (2019 for GPT-2).\n",
        "\n",
        "6. **Lack of True Understanding**: While text appears coherent, the model doesn't truly comprehend the concepts it's discussing."
      ],
      "metadata": {
        "id": "wHkHcu5rAawS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6.3 Educational Value Demonstrated*\n",
        "\n",
        "This practical work successfully illustrates:\n",
        "\n",
        "- Tokenization process and how text is processed by AI models\n",
        "\n",
        "- Parameter effects on generation quality and creativity\n",
        "\n",
        "- Trade-offs between coherence and diversity in AI text generation\n",
        "\n",
        "- Real-world limitations of current language models\n",
        "\n",
        "- Practical skills in manipulating and configuring AI systems"
      ],
      "metadata": {
        "id": "pDyOBLGpB5Ud"
      }
    }
  ]
}